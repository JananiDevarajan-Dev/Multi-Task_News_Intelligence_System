{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a30bbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# IMPORTS\n",
    "# ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb5fbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# TEXT CLEANING FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "import string\n",
    "def clean_text(text):\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove HTML\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = re.sub(\n",
    "        \"[\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"]+\", \n",
    "        \"\", \n",
    "        text\n",
    "    )\n",
    "\n",
    "    # Remove special symbols\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \" \", text)\n",
    "\n",
    "    # Keep only allowed characters\n",
    "    allowed = set(string.ascii_letters + string.digits + \" .,!?\")\n",
    "    text = \"\".join(ch for ch in text if ch in allowed)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "607049b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ---------------------------------------------------------\n",
    "df = pd.read_csv(\"D:/Python_WC/Final_project/Multi-Task_News_Intelligence_System/Data/news.tsv\", sep=\"\\t\")\n",
    "\n",
    "df[\"text\"] = df[\"Headline\"].fillna(\"\") + \" \" + df[\"News body\"].fillna(\"\")\n",
    "df = df.rename(columns={\"Category\": \"label\"}).dropna()\n",
    "\n",
    "# Clean text\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# This is required for evaluation\n",
    "df[\"full_text\"] = df[\"text\"]   # FIXED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "235f4a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization & Stopword Handling\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8cae273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence & word tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if w not in stop_words and w.isalpha()]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "213ffdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply tokenization (optional column)\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize_and_remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87f19ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncate text lengths\n",
    "MAX_ARTICLE_TOKENS = 512\n",
    "MAX_SUMMARY_TOKENS = 128\n",
    "\n",
    "def truncate_text(text, max_tokens):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = tokens[:max_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"article_trunc\"] = df[\"clean_text\"].apply(\n",
    "    lambda x: truncate_text(x, MAX_ARTICLE_TOKENS)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e1c7cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         predicting atlanta united s lineup against col...\n",
       "1         mitch mcconnell dc statehood push is full bore...\n",
       "2         home in north highlands damaged by fire north ...\n",
       "3         meghan mccain blames liberal media and third w...\n",
       "4         today in history aug 1 1714 george i becomes k...\n",
       "                                ...                        \n",
       "113757    hope who ? alyssa naeher s penalty save sends ...\n",
       "113758    chris sale explains what specifically has gone...\n",
       "113759    raptor fans jam streets to celebrate 1st nba t...\n",
       "113760    judge won t allow flynn to fire his attorneys ...\n",
       "113761    worley thinks he and conley will rival greates...\n",
       "Name: article_trunc, Length: 113704, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"article_trunc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eb31dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization / Feature Representations\n",
    "#TF-IDF (for extractive summarization)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf614415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extractive Summarization (TF-IDF Baseline)\n",
    "#TF-IDF sentence scorer\n",
    "import numpy as np\n",
    "\n",
    "def tfidf_extractive_summary(text, num_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    sentence_scores = np.array(tfidf.sum(axis=1)).flatten()\n",
    "    top_sentence_indices = sentence_scores.argsort()[-num_sentences:]\n",
    "    top_sentence_indices.sort()\n",
    "\n",
    "    summary = \" \".join([sentences[i] for i in top_sentence_indices])\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f78a4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate summaries\n",
    "df[\"tfidf_summary\"] = df[\"full_text\"].apply(\n",
    "    lambda x: tfidf_extractive_summary(x, num_sentences=3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "561d6280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec (Static Embeddings)\n",
    "#Train Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = df[\"tokens\"].tolist()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4903c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sentence vectors (average embeddings)\n",
    "import numpy as np\n",
    "\n",
    "def sentence_vector(sentence_tokens, model, vector_size=100):\n",
    "    vectors = [\n",
    "        model.wv[word]\n",
    "        for word in sentence_tokens\n",
    "        if word in model.wv\n",
    "    ]\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "df[\"w2v_vector\"] = df[\"tokens\"].apply(\n",
    "    lambda x: sentence_vector(x, w2v_model)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "829f5891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhiya\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Transformer Summarization (BART)\n",
    "#Load BART summarizer\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-base\",\n",
    "    tokenizer=\"facebook/bart-base\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ee52c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extractive Baseline (TF-IDF-based)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#TF-IDF Sentence Scoring\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def tfidf_extractive_summary(text, top_k=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) <= top_k:\n",
    "        return text\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    scores = tfidf_matrix.sum(axis=1).A1\n",
    "    top_sentence_ids = scores.argsort()[-top_k:]\n",
    "    top_sentence_ids.sort()\n",
    "\n",
    "    summary = \" \".join([sentences[i] for i in top_sentence_ids])\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3688c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Extractive Summaries\n",
    "df[\"tfidf_summary\"] = df[\"full_text\"].apply(\n",
    "    lambda x: tfidf_extractive_summary(x, top_k=3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1b95104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         We've seen how he rotates (or doesn't rotate) ...\n",
       "1         Mitch McConnell: DC statehood push is 'full bo...\n",
       "2         Home In North Highlands Damaged By Fire NORTH ...\n",
       "3         Meghan McCain blames 'liberal media' and 'thir...\n",
       "4         1798: Battle of Nile begins Battle of Nile, al...\n",
       "                                ...                        \n",
       "113757    No, when the final whistle sounded, the entire...\n",
       "113758    In his last start before the All-Star break, S...\n",
       "113759    Raptor fans jam streets to celebrate 1st NBA t...\n",
       "113760    Attorneys for President Trump's former nationa...\n",
       "113761    The kind of season they had overall lends litt...\n",
       "Name: tfidf_summary, Length: 113704, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tfidf_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f1bb380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# TEXT RANK SUMMARIZER\n",
    "# ---------------------------------------------------------\n",
    "def textrank_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    ranked = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    summary = \" \".join([s for _, s in ranked[:top_n]])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9f893e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# TF-IDF SENTENCE SCORING\n",
    "# ---------------------------------------------------------\n",
    "def tfidf_summarize(text, top_n=3):\n",
    "    cleaned = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned)\n",
    "\n",
    "    if len(sentences) <= top_n:\n",
    "        return \" \".join(sentences)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    scores = tfidf_matrix.mean(axis=1).A.flatten()\n",
    "\n",
    "    ranked_idx = np.argsort(scores)[::-1]\n",
    "    selected = [sentences[i] for i in ranked_idx[:top_n]]\n",
    "\n",
    "    return \" \".join(selected)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# REFERENCE SUMMARY (WEAK BASELINE)\n",
    "# ---------------------------------------------------------\n",
    "def reference_summary(text):\n",
    "    sents = sent_tokenize(clean_text(text))\n",
    "    return \" \".join(sents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88e642a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ROUGE EVALUATION\n",
    "# ---------------------------------------------------------\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def evaluate_model(summarizer_fn, df, samples=50):\n",
    "\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    for i in range(min(samples, len(df))):\n",
    "\n",
    "        text = df.iloc[i][\"full_text\"]\n",
    "        ref = reference_summary(text)\n",
    "        pred = summarizer_fn(text)\n",
    "\n",
    "        scores = scorer.score(ref, pred)\n",
    "\n",
    "        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": np.mean(rouge1_scores),\n",
    "        \"rouge2\": np.mean(rouge2_scores),\n",
    "        \"rougeL\": np.mean(rougeL_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e3e6d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TextRank...\n",
      "Evaluating TF-IDF...\n",
      "\n",
      "Evaluation Completed.\n",
      "Saved to: rouge_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# RUN EVALUATION\n",
    "# ---------------------------------------------------------\n",
    "print(\"Evaluating TextRank...\")\n",
    "textrank_scores = evaluate_model(textrank_summarize, df)\n",
    "\n",
    "print(\"Evaluating TF-IDF...\")\n",
    "tfidf_scores = evaluate_model(tfidf_summarize, df)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SAVE RESULTS TO CSV\n",
    "# ---------------------------------------------------------\n",
    "OUTPUT_CSV = \"rouge_eval_results.csv\"  # change if needed\n",
    "\n",
    "new_results = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"TextRank\",\n",
    "        \"rouge1\": textrank_scores[\"rouge1\"],\n",
    "        \"rouge2\": textrank_scores[\"rouge2\"],\n",
    "        \"rougeL\": textrank_scores[\"rougeL\"],\n",
    "        \"rougeLsum\": textrank_scores[\"rougeL\"],\n",
    "        \"Average Score\": np.mean([\n",
    "            textrank_scores[\"rouge1\"],\n",
    "            textrank_scores[\"rouge2\"],\n",
    "            textrank_scores[\"rougeL\"],\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"TF-IDF\",\n",
    "        \"rouge1\": tfidf_scores[\"rouge1\"],\n",
    "        \"rouge2\": tfidf_scores[\"rouge2\"],\n",
    "        \"rougeL\": tfidf_scores[\"rougeL\"],\n",
    "        \"rougeLsum\": tfidf_scores[\"rougeL\"],\n",
    "        \"Average Score\": np.mean([\n",
    "            tfidf_scores[\"rouge1\"],\n",
    "            tfidf_scores[\"rouge2\"],\n",
    "            tfidf_scores[\"rougeL\"],\n",
    "        ])\n",
    "    }\n",
    "])\n",
    "\n",
    "new_results = new_results.round(4)\n",
    "\n",
    "# If CSV exists → append, else create new\n",
    "try:\n",
    "    old_df = pd.read_csv(OUTPUT_CSV)\n",
    "    final_df = pd.concat([old_df, new_results], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    final_df = new_results\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"\\nEvaluation Completed.\\nSaved to:\", OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c20caac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextRank</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>0.3924</td>\n",
       "      <td>0.4402</td>\n",
       "      <td>0.4402</td>\n",
       "      <td>0.4451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.4836</td>\n",
       "      <td>0.3779</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.4253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  rouge1  rouge2  rougeL  rougeLsum  Average Score\n",
       "0  TextRank  0.5028  0.3924  0.4402     0.4402         0.4451\n",
       "1    TF-IDF  0.4836  0.3779  0.4144     0.4144         0.4253"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1=pd.read_csv(\"D:/Python_WC/Final_project/Multi-Task_News_Intelligence_System/Summarization/rouge_eval_results.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3b3f9",
   "metadata": {},
   "source": [
    "*DL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b82aa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# TEXT CLEANING FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "def dl_clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)                      # Remove HTML\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)   # Remove URLs\n",
    "\n",
    "    # Remove emojis\n",
    "    text = re.sub(\"[\" \n",
    "                  u\"\\U0001F600-\\U0001F64F\"\n",
    "                  u\"\\U0001F300-\\U0001F5FF\"\n",
    "                  u\"\\U0001F680-\\U0001F6FF\"\n",
    "                  u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                  \"]+\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \" \", text)        # Special chars\n",
    "    allowed = set(string.ascii_letters + string.digits + \" .,!?\")\n",
    "    text = \"\".join(ch for ch in text if ch in allowed)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4031a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# LOAD DATA (PENS)\n",
    "# ---------------------------------------------------------\n",
    "df = pd.read_csv(\"../data/news.tsv\", sep=\"\\t\")\n",
    "\n",
    "df[\"article\"] = df[\"News body\"].fillna(\"\").apply(dl_clean_text)\n",
    "df[\"summary\"] = df[\"Headline\"].fillna(\"\").apply(dl_clean_text)\n",
    "\n",
    "df = df[(df[\"article\"].str.len() > 0) & (df[\"summary\"].str.len() > 0)]\n",
    "\n",
    "# Add special tokens\n",
    "df[\"summary_in\"]  = \"<sos> \" + df[\"summary\"]\n",
    "df[\"summary_out\"] = df[\"summary\"] + \" <eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcbb8f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30000\n"
     ]
    }
   ],
   "source": [
    "#Prepare Tokenizer (Seq2Seq)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "MAX_ART_LEN = 500     # encoder input length\n",
    "MAX_SUM_LEN = 50      # decoder input/output length\n",
    "VOCAB_SIZE = 30000\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=VOCAB_SIZE,\n",
    "    oov_token=\"<unk>\",\n",
    "    filters=\"\"   # VERY IMPORTANT: keeps <sos> and <eos>\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(\n",
    "    df[\"article\"].tolist() +\n",
    "    df[\"summary_in\"].tolist() +\n",
    "    df[\"summary_out\"].tolist()\n",
    ")\n",
    "encoder_input_seq = tokenizer.texts_to_sequences(df[\"article\"])\n",
    "decoder_input_seq = tokenizer.texts_to_sequences(df[\"summary_in\"])\n",
    "decoder_output_seq = tokenizer.texts_to_sequences(df[\"summary_out\"])\n",
    "encoder_input_seq = pad_sequences(\n",
    "    encoder_input_seq,\n",
    "    maxlen=MAX_ART_LEN,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "\n",
    "decoder_input_seq = pad_sequences(\n",
    "    decoder_input_seq,\n",
    "    maxlen=MAX_SUM_LEN,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "\n",
    "decoder_output_seq = pad_sequences(\n",
    "    decoder_output_seq,\n",
    "    maxlen=MAX_SUM_LEN,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "vocab_size = min(VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e859b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Encoder–Decoder with Attention (LSTM)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Dense, Attention, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#Model hyperparameters\n",
    "EMB_DIM = 128\n",
    "LATENT_DIM = 256\n",
    "\n",
    "#Encoder\n",
    "# Encoder input\n",
    "encoder_inputs = Input(shape=(MAX_ART_LEN,), name=\"encoder_inputs\")\n",
    "\n",
    "# Encoder embedding\n",
    "encoder_embedding = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=EMB_DIM,\n",
    "    mask_zero=True,\n",
    "    name=\"encoder_embedding\"\n",
    ")(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM\n",
    "encoder_lstm = LSTM(\n",
    "    LATENT_DIM,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"encoder_lstm\"\n",
    ")\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "#Decoder\n",
    "# Decoder input\n",
    "decoder_inputs = Input(shape=(MAX_SUM_LEN,), name=\"decoder_inputs\")\n",
    "\n",
    "# Decoder embedding\n",
    "decoder_embedding = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=EMB_DIM,\n",
    "    mask_zero=True,\n",
    "    name=\"decoder_embedding\"\n",
    ")(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(\n",
    "    LATENT_DIM,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder_lstm\"\n",
    ")\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    decoder_embedding,\n",
    "    initial_state=[state_h, state_c]\n",
    ")\n",
    "\n",
    "#Attention Layer\n",
    "attention = Attention(name=\"attention_layer\")\n",
    "\n",
    "context_vector = attention(\n",
    "    [decoder_outputs, encoder_outputs]\n",
    ")\n",
    "#Concatenate + Output\n",
    "decoder_concat = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_outputs, context_vector]\n",
    ")\n",
    "\n",
    "decoder_dense = Dense(\n",
    "    vocab_size,\n",
    "    activation=\"softmax\",\n",
    "    name=\"output_layer\"\n",
    ")\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_concat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e8b4675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ encoder_embeddin… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ decoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390,000</span> │ concat_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m3,840,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m3,840,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m,      │    \u001b[38;5;34m394,240\u001b[0m │ encoder_embeddin… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m394,240\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_layer[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m30000\u001b[0m) │ \u001b[38;5;34m15,390,000\u001b[0m │ concat_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,858,480</span> (91.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,858,480\u001b[0m (91.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,858,480</span> (91.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,858,480\u001b[0m (91.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Build & compile model\n",
    "model = Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs],\n",
    "    outputs=decoder_outputs\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6f18329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3198/3198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2617s\u001b[0m 817ms/step - accuracy: 0.1572 - loss: 6.5085 - val_accuracy: 0.1935 - val_loss: 5.7941\n",
      "Epoch 2/5\n",
      "\u001b[1m3198/3198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2234s\u001b[0m 698ms/step - accuracy: 0.2205 - loss: 5.3112 - val_accuracy: 0.2331 - val_loss: 5.2236\n",
      "Epoch 3/5\n",
      "\u001b[1m3198/3198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2444s\u001b[0m 764ms/step - accuracy: 0.2618 - loss: 4.6473 - val_accuracy: 0.2542 - val_loss: 4.9789\n",
      "Epoch 4/5\n",
      "\u001b[1m3198/3198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2341s\u001b[0m 732ms/step - accuracy: 0.2975 - loss: 4.1499 - val_accuracy: 0.2664 - val_loss: 4.8762\n",
      "Epoch 5/5\n",
      "\u001b[1m3198/3198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2419s\u001b[0m 756ms/step - accuracy: 0.3330 - loss: 3.7350 - val_accuracy: 0.2738 - val_loss: 4.8572\n"
     ]
    }
   ],
   "source": [
    "#Training with Teacher Forcing\n",
    "#Shift decoder output\n",
    "decoder_target_seq = decoder_output_seq[..., None]\n",
    "#Train model\n",
    "history = model.fit(\n",
    "    [encoder_input_seq, decoder_input_seq],\n",
    "    decoder_target_seq,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3732f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved seq2seq_lstm_model.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "#Save the trained Seq2Seq model\n",
    "model.save('seq2seq_lstm_model.h5')\n",
    "print(\"✅ Saved seq2seq_lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e26faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the tokenizer\n",
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab980d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save training history (loss & accuracy)\n",
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(\"training_history_DL.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76faab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model config info\n",
    "config = {\n",
    "    \"MAX_ART_LEN\": MAX_ART_LEN,\n",
    "    \"MAX_SUM_LEN\": MAX_SUM_LEN,\n",
    "    \"VOCAB_SIZE\": vocab_size,\n",
    "    \"EMB_DIM\": 128,\n",
    "    \"LATENT_DIM\": 256\n",
    "}\n",
    "\n",
    "with open(\"model_config_DL.json\", \"w\") as f:\n",
    "    import json\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eeb50bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq summary generation function\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "\n",
    "# ARTICLE TOKENIZER\n",
    "article_tokenizer = Tokenizer(\n",
    "    num_words=30000,\n",
    "    oov_token=\"<unk>\"\n",
    ")\n",
    "article_tokenizer.fit_on_texts(df[\"article\"])\n",
    "\n",
    "\n",
    "# SUMMARY TOKENIZER (IMPORTANT FIX)\n",
    "summary_tokenizer = Tokenizer(\n",
    "    num_words=10000,\n",
    "    filters=\"\"   # do NOT remove <sos> or <eos>\n",
    ")\n",
    "\n",
    "# Fit on BOTH summary_in and summary_out\n",
    "summary_tokenizer.fit_on_texts(\n",
    "    df[\"summary_in\"].tolist() + df[\"summary_out\"].tolist()\n",
    ")\n",
    "\n",
    "# Save tokenizers\n",
    "with open(\"article_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(article_tokenizer, f)\n",
    "\n",
    "with open(\"summary_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(summary_tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf580f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"<sos>\" in summary_tokenizer.word_index)\n",
    "print(\"<eos>\" in summary_tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c87af4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ encoder_embeddin… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m3,840,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m,      │    \u001b[38;5;34m394,240\u001b[0m │ encoder_embeddin… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,234,240</span> (16.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,234,240\u001b[0m (16.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,234,240</span> (16.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,234,240\u001b[0m (16.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Encoder Inference Model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model = Model(\n",
    "    inputs=model.input[0],                    # encoder_inputs\n",
    "    outputs=model.get_layer(\"encoder_lstm\").output\n",
    ")\n",
    "\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4307692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "(1, 500, 256)\n"
     ]
    }
   ],
   "source": [
    "seq = article_tokenizer.texts_to_sequences([df[\"article\"].iloc[0]])\n",
    "seq = pad_sequences(seq, maxlen=MAX_ART_LEN, padding=\"post\")\n",
    "\n",
    "encoder_outputs, h, c = encoder_model.predict(seq)\n",
    "\n",
    "print(encoder_outputs.shape)  # MUST be (1, 50, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64942b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input_token │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ decoder_input_to… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_h           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_c           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ decoder_embeddin… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ decoder_h[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ decoder_c[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_outputs     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ encoder_outputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ attention_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390,000</span> │ concat_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input_token │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │  \u001b[38;5;34m3,840,000\u001b[0m │ decoder_input_to… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_h           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_c           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m),  │    \u001b[38;5;34m394,240\u001b[0m │ decoder_embeddin… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ decoder_h[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ decoder_c[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_outputs     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m4\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ encoder_outputs[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concat_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m4\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ attention_layer[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m30000\u001b[0m)  │ \u001b[38;5;34m15,390,000\u001b[0m │ concat_layer[\u001b[38;5;34m3\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,624,240</span> (74.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,624,240\u001b[0m (74.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,624,240</span> (74.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,624,240\u001b[0m (74.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Decoder Inference Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Decoder inputs\n",
    "decoder_inputs = Input(shape=(1,), name=\"decoder_input_token\")\n",
    "decoder_state_input_h = Input(shape=(256,), name=\"decoder_h\")\n",
    "decoder_state_input_c = Input(shape=(256,), name=\"decoder_c\")\n",
    "\n",
    "# ✅ FIXED HERE\n",
    "encoder_outputs_input = Input(shape=(None, 256), name=\"encoder_outputs\")\n",
    "\n",
    "# Layers from trained model\n",
    "decoder_embedding = model.get_layer(\"decoder_embedding\")\n",
    "decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
    "attention_layer = model.get_layer(\"attention_layer\")\n",
    "concat_layer = model.get_layer(\"concat_layer\")\n",
    "output_layer = model.get_layer(\"output_layer\")\n",
    "\n",
    "# Forward pass\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(\n",
    "    decoder_embedded,\n",
    "    initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "\n",
    "attention_output = attention_layer(\n",
    "    [decoder_outputs, encoder_outputs_input]\n",
    ")\n",
    "\n",
    "decoder_concat = concat_layer([decoder_outputs, attention_output])\n",
    "decoder_outputs = output_layer(decoder_concat)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoder_outputs_input,\n",
    "        decoder_state_input_h,\n",
    "        decoder_state_input_c\n",
    "    ],\n",
    "    [decoder_outputs, h, c]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "decoder_model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0cc2b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seq2Seq Summary Generation\n",
    "def seq2seq_generate_summary(text):\n",
    "    seq = article_tokenizer.texts_to_sequences([text])\n",
    "    seq = pad_sequences(seq, maxlen=MAX_ART_LEN, padding=\"post\")\n",
    "\n",
    "    encoder_outputs, h, c = encoder_model.predict(seq, verbose=0)\n",
    "\n",
    "    start_token = summary_tokenizer.word_index[\"<sos>\"]\n",
    "    end_token   = summary_tokenizer.word_index[\"<eos>\"]\n",
    "\n",
    "    target_seq = np.array([[start_token]])\n",
    "    decoded_words = []\n",
    "\n",
    "    for _ in range(MAX_SUM_LEN):\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs, h, c],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = summary_tokenizer.index_word.get(sampled_token_index, \"\")\n",
    "\n",
    "        if sampled_word == \"<eos>\" or sampled_word == \"\":\n",
    "            break\n",
    "\n",
    "        decoded_words.append(sampled_word)\n",
    "        target_seq = np.array([[sampled_token_index]])\n",
    "\n",
    "    return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "884c91a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE:\n",
      "only five internationals allowed, count em, five! so first off we should say, per our usual atlanta united lineup predictions, this will be wrong. why will it be wrong? well, aside from the obvious, we still don t have a ton of data points from frank de boer in how he prefers to rotate his team for \n",
      "\n",
      "REFERENCE SUMMARY:\n",
      "predicting atlanta united s lineup against columbus crew in the u.s. open cup\n",
      "\n",
      "SEQ2SEQ SUMMARY:\n",
      "the still after\n"
     ]
    }
   ],
   "source": [
    "#Test It\n",
    "print(\"ARTICLE:\")\n",
    "print(df[\"article\"].iloc[0][:300])\n",
    "\n",
    "print(\"\\nREFERENCE SUMMARY:\")\n",
    "print(df[\"summary\"].iloc[0])\n",
    "\n",
    "print(\"\\nSEQ2SEQ SUMMARY:\")\n",
    "print(seq2seq_generate_summary(df[\"article\"].iloc[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47895b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:52<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "#ROUGE evaluation\n",
    "#Generate Seq2Seq summaries for your test set\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "MAX_TEST = 100  # how many examples to evaluate (for speed; use all if you want)\n",
    "\n",
    "seq2seq_summaries = []\n",
    "\n",
    "for i in tqdm(range(min(MAX_TEST, len(df)))):\n",
    "    article = df[\"article\"].iloc[i]\n",
    "    summary_pred = seq2seq_generate_summary(article)\n",
    "    seq2seq_summaries.append(summary_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e93442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare reference summaries\n",
    "references = df[\"summary\"].iloc[:MAX_TEST].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "11d4d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1   : 0.0276\n",
      "ROUGE-2   : 0.0000\n",
      "ROUGE-L   : 0.0264\n"
     ]
    }
   ],
   "source": [
    "#ROUGE evaluation\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "# Score each prediction\n",
    "for ref, pred in zip(references, seq2seq_summaries):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    aggregator.add_scores(scores)\n",
    "\n",
    "# Get aggregate scores\n",
    "result = aggregator.aggregate()\n",
    "\n",
    "print(\"ROUGE-1   : {:.4f}\".format(result['rouge1'].mid.fmeasure))\n",
    "print(\"ROUGE-2   : {:.4f}\".format(result['rouge2'].mid.fmeasure))\n",
    "print(\"ROUGE-L   : {:.4f}\".format(result['rougeL'].mid.fmeasure))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "140f44f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Completed.\n",
      "Saved to: rouge_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# SAVE RESULTS TO CSV\n",
    "# ---------------------------------------------------------\n",
    "OUTPUT_CSV = \"rouge_eval_results.csv\"  # change if needed\n",
    "\n",
    "new_results = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Seq2Seq LSTM\",\n",
    "        \"rouge1\":result[\"rouge1\"].mid.fmeasure,\n",
    "        \"rouge2\": result[\"rouge2\"].mid.fmeasure,\n",
    "        \"rougeL\": result[\"rougeL\"].mid.fmeasure,\n",
    "        \"rougeLsum\": result[\"rougeL\"].mid.fmeasure,\n",
    "        \"Average Score\": np.mean([\n",
    "            result[\"rouge1\"].mid.fmeasure,\n",
    "            result[\"rouge2\"].mid.fmeasure,\n",
    "            result[\"rougeL\"].mid.fmeasure,\n",
    "        ])\n",
    "    } \n",
    "])\n",
    "\n",
    "new_results = new_results.round(4)\n",
    "\n",
    "# If CSV exists → append, else create new\n",
    "try:\n",
    "    old_df = pd.read_csv(OUTPUT_CSV)\n",
    "    final_df = pd.concat([old_df, new_results], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    final_df = new_results\n",
    "\n",
    "final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(\"\\nEvaluation Completed.\\nSaved to:\", OUTPUT_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
